# Safe, Robust & Explainable Reinforcement Learning

### Phase 6 of the Reinforcement Learning Roadmap

This repository contains the slides, Jupyter notebook, and curated resources for **Phase 6** of my Reinforcement Learning learning roadmap, focusing on **trustworthy deep RL** â€” the engineering of agents that are safe under constraints, robust to adversarial perturbations, and transparent enough for regulatory audit.

ğŸ“Œ **Author:** Abdullah Zahid  
ğŸ“… **Date:** February 2026  
ğŸ”— **LinkedIn:** https://www.linkedin.com/in/abdullahzahid655  
ğŸ™ **GitHub:** https://github.com/abdullahzahid655

---

## ğŸ“˜ Contents

### Slides
- **Phase 06 â€“ Safety, Robustness & Explainability**  
  10-slide LinkedIn carousel covering:
  - Constrained Markov Decision Processes (CMDP) and Lagrangian dual optimisation
  - State-Adversarial MDPs (SA-MDP) and SA-DQN adversarial training
  - SHapley Additive exPlanations (SHAP) applied to Q-networks
  - Policy distillation into interpretable decision trees
  - Performance benchmarking dashboard across all three agents
  - Seminal literature and practitioner exercise sets

ğŸ“„ Location: `slides/Phase_06_Safe_Robust_Explainable_RL.pptx`

### Notebook
- **Phase5_6_Safe_Robust_Explainable_RL.ipynb** â€” 26-cell end-to-end implementation:
  - Safe CartPole CMDP wrapper with cost signal and safety budget
  - Behaviour Cloning offline pretraining (Phase 5)
  - Baseline DQN fine-tuned from BC weights
  - Safe DQN with dual Q-heads and adaptive Lagrange multiplier Î»
  - Robust DQN with FGSM adversarial training and consistency regularisation
  - KernelSHAP attribution on Q-network
  - Decision tree policy distillation with IF-THEN rule extraction
  - Comparison dashboard across all three agents

ğŸ“„ Location: `Phase5_6_Safe_Robust_Explainable_RL.ipynb`

---

## ğŸ§  Topics Covered

### 1. Safe Reinforcement Learning â€” Constrained MDPs

The standard RL objective is extended with a cost signal and a safety budget:

```
max  ğ”¼Ï€ [ Î£ Î³áµ— râ‚œ ]     subject to     ğ”¼Ï€ [ Î£ Î³áµ— câ‚œ ] â‰¤ b
```

Solved via Lagrangian dual optimisation:

```
â„’(Ï€, Î») = JÊ³ âˆ’ Î»(Já¶œ âˆ’ b)
```

where Î» â†‘ when cumulative cost exceeds budget, and Î» â†“ otherwise.

**Algorithm families covered:**
- Lagrangian Methods: PPO-Lagrangian, TRPO-Lagrangian, PID-Lagrangian
- Trust-Region Methods: CPO (Achiam et al., ICML 2017), PCPO
- Model-Based Safety: SafeDreamer (Huang et al., ICLR 2024)
- Hard Safety Guarantees: Control Barrier Functions (CBF)

**Industry deployments:**
- Autonomous Driving (Waymo / Tesla): collision probability < 10â»â¶ per mile
- Power Grid Management: CVaR constraints on reliability
- Robotic Arm Manipulation: CMDP-based hazard avoidance (Robotics, MDPI 2024)
- Multi-Agent Drone Swarms: Scal-MAPPO-L (NeurIPS 2024)

---

### 2. Robust Reinforcement Learning â€” State-Adversarial MDPs

DRL policies are brittle: small observation perturbations collapse performance. The **State-Adversarial MDP (SA-MDP)** formalises this:

```
SA-MDP:  Î©Ë¢ = (S, A, T, R, ğ’³, O^Î¾)
```

The adversary modifies observations: O^Î¾(xâ‚œ | sâ‚œ). The agent must perform well under **worst-case** perturbations.

**Adversarial attack taxonomy:**
- Observation Attacks: FGSM, PGD variants
- Action Attacks: NR-MDP framework
- Reward Attacks: Reward poisoning
- Adversarial Policies in MARL: Gleave et al., ICLR 2020

**SA-DQN (Zhang et al., NeurIPS 2020):**

```
FGSM:  x_adv = x + Îµ Â· sign(âˆ‡â‚“ L(Î¸, x, a))

Robustness Loss:  L = L_TD(clean) + Î± Â· â€–Q(x) âˆ’ Q(x_adv)â€–
```

Hyperparameters: Îµ = 0.05, Î± = 0.5  
**RADIAL-RL** (Oikarinen et al., NeurIPS 2021) extends this with certified bounds via interval bound propagation (IBP).

---

### 3. Explainable RL â€” SHapley Additive exPlanations

XRL is the subfield that elucidates RL decision-making, enabling practitioners to understand *what* agents will do and *why* [Milani et al., ACM 2023].

**Shapley value attribution:**

```
Ï†áµ¢ = Î£_{S âŠ† F\{i}}  |S|!(|F|âˆ’|S|âˆ’1)! / |F|!  Â·  [v(S âˆª {i}) âˆ’ v(S)]
```

Ï†áµ¢ > 0: feature i increased action value Â· Ï†áµ¢ < 0: feature i suppressed action value

**XRL Taxonomy (Milani et al., 2023):**

| Category | Methods |
|----------|---------|
| Feature Importance | SHAP Values Â· Saliency Maps Â· Gradient-based |
| Learning Process | Experience Attribution Â· Reward Attribution |
| Policy-Level | Decision Tree Distillation Â· NL Explanations |
| Counterfactual | Minimal state change to flip the decision |

**CartPole feature importance (SHAP):**

| Feature | Importance |
|---------|-----------|
| Pole Angle (Î¸) | 92% |
| Pole Angular Velocity (Î¸Ì‡) | 74% |
| Cart Position (x) | 28% |
| Cart Velocity (áº‹) | 18% |

---

### 4. Policy Distillation â€” Interpretable Surrogate Models

```
Steps:
1. Train high-performing DNN policy Ï€DNN
2. Generate dataset: (s, Ï€DNN(s)) pairs â€” 300 episodes
3. Fit DecisionTreeClassifier (sklearn, max_depth=4)
4. Fidelity = P[ Tree(s) == DNN(s) ]   â†’  target: >90%
5. Extract rules: export_text(tree, feature_names=[...])
```

Sample extracted rule:
```
IF  Î¸Ì‡ â‰¥ 0.021  AND  Î¸ â‰¥ 0.004  â†’  PUSH RIGHT  (action = 1)
```

---

## ğŸ“Š Results

### Agent Benchmarking

| Agent | Method | Phase | Avg Reward â†‘ | Violations â†“ | Noise Robust |
|-------|--------|-------|-------------|-------------|--------------|
| Baseline DQN | DQN + Behaviour Cloning | 5 | **420** | 32 âŒ | Low |
| Safe DQN | Lagrangian CMDP | 6 | 378 | **8 âœ…** | Medium |
| Robust DQN | SA-DQN / FGSM | 6 | 352 | 27 âš ï¸ | **High âœ…** |

### Key Metrics

| Metric | Value | Context |
|--------|-------|---------|
| Constraint Violation Reduction | **75%** | Safe DQN vs. Baseline DQN |
| Reward Retention Under Noise Ïƒ=0.3 | **+40%** | Robust DQN vs. Baseline DQN |
| Decision Tree Fidelity | **>90%** | Tree surrogate vs. DNN policy |
| Lagrange Multiplier Peak Growth | **2.1Ã—** | Over 500 training episodes |

---

## ğŸ— System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SafeCartPoleEnv (CMDP Wrapper)         â”‚
â”‚  cost = 1 if |Î¸| > 0.15 rad  Â·  budget b = 20/episode  â”‚
â”‚  Gaussian noise injection (Ïƒ-adjustable)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼               â–¼               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ DQNAgent  â”‚   â”‚ SafeDQN   â”‚   â”‚ RobustDQN â”‚
 â”‚ +BC pretrainâ”‚  â”‚ 2 Q-heads â”‚   â”‚ FGSM+L_robâ”‚
 â”‚ (Phase 5) â”‚   â”‚ Î» adaptiveâ”‚   â”‚ (SA-DQN)  â”‚
 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚KernelSHAP â”‚                  â”‚Decision Tree â”‚
 â”‚ Ï†áµ¢ per    â”‚                  â”‚Distillation  â”‚
 â”‚ state-stepâ”‚                  â”‚IF-THEN rules â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training pipeline:**
Offline BC Pretraining â†’ DQN Fine-Tuning â†’ Safe / Robust Training â†’ SHAP Attribution â†’ Decision Tree Distillation

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/abdullahzahid655/RL-Phase6-Safe-Robust-Explainable.git
cd RL-Phase6-Safe-Robust-Explainable
pip install gymnasium numpy torch matplotlib seaborn shap scikit-learn pandas tqdm
jupyter notebook Phase5_6_Safe_Robust_Explainable_RL.ipynb
```

---

## ğŸ“š Key References

**Safe RL:**
- Achiam et al. (2017). *Constrained Policy Optimization.* ICML.
- GarcÃ­a & FernÃ¡ndez (2015). *Comprehensive Survey on Safe RL.* JMLR 16(1).
- Huang et al. (2024). *SafeDreamer.* ICLR. arXiv:2307.07176
- Wachi et al. (2024). *Survey on Constraint Formulations.* arXiv:2402.02025
- Liu et al. (2024). *FISOR: Feasibility-guided Safe Offline RL.* ICLR.

**Robust RL:**
- Zhang et al. (2020). *SA-DQN.* NeurIPS Spotlight.
- Oikarinen et al. (2021). *RADIAL-RL.* NeurIPS.
- Gleave et al. (2020). *Adversarial Policies.* ICLR.

**Explainable RL:**
- Milani et al. (2023). *XRL Survey.* ACM Computing Surveys.
- Beechey et al. (2023). *SHAP for RL.* ICML.
- Bekkemoen (2024). *XRL Systematic Review.* Machine Learning 113.

**Offline RL (Phase 5):**
- Levine et al. (2020). *Offline RL Tutorial.* arXiv:2005.01643
- Kumar et al. (2020). *CQL.* NeurIPS.

See `resources/papers.md` for complete references with arXiv links.

---

## ğŸ›  Implementation Resources

- OmniSafe â€” Safe RL algorithm library
- Safety-Gymnasium â€” Unified safe RL benchmark
- SA-DQN codebase â€” github.com/chenhongge/SA_DQN
- RADIAL-RL â€” github.com/tuomaso/radial_rl_v2
- SHAP â€” `pip install shap`
- Captum â€” PyTorch native XAI
- PettingZoo â€” Multi-agent RL environments
- DSRL â€” Offline safe RL datasets

See `resources/libraries.md` for the complete tooling guide.

---

## ğŸ—º Roadmap Context

This is **Phase 6** of a larger RL roadmap:

- Phase 1â€“2: Fundamentals & Deep RL Architecture
- Phase 3â€“4: Applications & Mathematical Foundations
- Phase 5: Advanced Paradigms (MARL, HRL, Meta-RL, Offline RL)
- **Phase 6: Safety, Robustness & Explainability (this repo)**
- Phase 7 (upcoming): Real-World Deployment, RLHF, Foundation Models

---

## ğŸ¤ Contributions & Feedback

This repository is shared for learning and discussion.  
Feedback, suggestions, and references are welcome.

If you find this useful, feel free to â­ the repository or share it.